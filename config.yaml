model:
  name: nanoGPT
  arch: GPT  # GPT, MEGABYTE, etc.
  context_size: 256
  n_layer: 6
  n_head: 6
  n_embed: 384
  dropout: 0.2

tokenizer:
  type: char  # char, bpe, etc.
  vocab_size: 0  # To be updated dynamically based on tokenizer

training:
  max_steps: 10000
  batch_size: 128
  optimizer:
    type: AdamW  # SGD, Adam, AdamW, etc.
    params:
      lr: 0.001
      weight_decay: 0.01
  scheduler:
    type: cosine
    warmup_ratio: 0
  grad_clip: 0
  mixed_precision: False
  validation_interval: 1000
  checkpoint_interval: 1000
  checkpoint_dir: checkpoints
  log_interval: 100
  log_file: logs/training.logs
  seed: 42

data:
  train_path: data/raw/shakespeare.txt
  val_size: 0.1

wandb:
  project: nanoGPT
  entity: pathfinderkr