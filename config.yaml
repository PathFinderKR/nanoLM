model:
  name: nanoGPT
  arch: GPT  # GPT, MEGABYTE, etc.
  context_size: 256
  n_layer: 6
  n_head: 6
  n_embed: 384
  dropout: 0.2

tokenizer:
  type: char  # char, bpe, etc.
  vocab_size: 67
  vocab_path: char_tokenizer.json

training:
  max_steps: 1000
  batch_size: 128
  optimizer:
    type: AdamW  # SGD, Adam, AdamW, etc.
    params:
      lr: 0.001
      weight_decay: 0.01
  scheduler:
    type: cosine
    warmup_ratio: 0
  grad_clip: 0
  mixed_precision: False
  validation_interval: 1000
  checkpoint_interval: 1000
  checkpoint_dir: checkpoints
  log_interval: 100
  log_file: logs/training.logs
  seed: 42

data:
  train_path: data/raw/shakespeare.txt
  val_size: 0.1

generation:
  model_path: checkpoints/nanoGPT/step_1000.pth
  prompt: "To be or not to be, that is the question."
  max_new_tokens: 1000
  temperature: 1.0
  log_file: logs/generation.logs
  seed: 42

wandb:
  project: nanoGPT
  entity: pathfinderkr